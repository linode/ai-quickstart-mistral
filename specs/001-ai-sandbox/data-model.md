# Data Model: One-Click AI Sandbox

**Date**: 2025-11-12  
**Feature**: One-Click AI Sandbox Marketplace App

## Overview

This Marketplace App deployment does not maintain a traditional database or data model. Instead, it manages:
- **Configuration state**: Deployment settings and service configuration
- **Persistent storage**: Model files and chat history on host filesystem
- **Runtime state**: Container service status

## Entities

### AI Model

**Purpose**: Represents the machine learning model used for inference.

**Attributes**:
- `model_identifier` (string, required): Hugging Face model ID
  - Default: `mistralai/Mistral-7B-Instruct-v0.3`
  - Format: `{org}/{model-name}`
  - Example: `mistralai/Mistral-7B-Instruct-v0.3`
- `download_status` (enum): Model download state
  - Values: `pending`, `downloading`, `completed`, `failed`
  - Stored in: Container logs, `/var/log/ai-sandbox/model-download.log`
- `configuration_state` (enum): Model configuration state
  - Values: `not_configured`, `configuring`, `ready`, `error`
  - Determined by: vLLM service health
- `active_status` (boolean): Whether model is currently active for inference
  - `true`: Model loaded and ready for requests
  - `false`: Model not loaded or service stopped

**Storage**: 
- Model files: `/opt/models` (host directory, persisted)
- Configuration: Environment variable `MODEL_ID` in docker-compose.yml
- Status: Determined by service health checks

**Lifecycle**:
1. **Deployment**: StackScript sets `MODEL_ID` environment variable
2. **Download**: vLLM container downloads model on first start (if not cached)
3. **Caching**: Model files cached in `/opt/models` for subsequent starts
4. **Activation**: Model loaded into GPU memory when vLLM service starts
5. **Runtime**: Model serves inference requests (sequential queue)

**Validation Rules**:
- Model ID must be valid Hugging Face format: `{org}/{model-name}`
- Model must be compatible with vLLM inference engine
- Model size must fit within instance GPU memory limits

---

### Chat Session

**Purpose**: Represents a conversation between a user and the AI model.

**Attributes**:
- `session_identifier` (string, auto-generated): Unique session ID
  - Generated by: Open WebUI
  - Format: UUID or similar unique identifier
  - Stored in: Open WebUI database (containerized)
- `message_history` (array): Conversation messages
  - Structure: Array of message objects with role, content, timestamp
  - Format: Open WebUI internal format
  - Stored in: Open WebUI database
- `timestamp` (datetime): Session creation/update time
  - Format: ISO 8601 or Unix timestamp
  - Stored in: Open WebUI database
- `persistence_state` (enum): Data persistence status
  - Values: `volatile`, `persisted`, `corrupted`
  - Determined by: Volume mount success, disk space availability

**Storage**: 
- Primary: Open WebUI container database (SQLite or similar)
- Volume mount: `/opt/open-webui` (host directory, persisted)
- Backup: Host directory ensures persistence across container restarts

**Lifecycle**:
1. **Creation**: User starts new conversation in chat UI
2. **Active**: User sends messages, receives AI responses
3. **Persistence**: Messages saved to Open WebUI database
4. **Retrieval**: Session history loaded on browser refresh
5. **Retention**: Persisted indefinitely (no automatic deletion in v1)

**Validation Rules**:
- Session must be associated with valid user interaction
- Messages must be non-empty (handled by Open WebUI)
- Timestamps must be sequential (handled by Open WebUI)

---

### Deployment Configuration

**Purpose**: Represents the deployment state and configuration.

**Attributes**:
- `instance_type` (string): Linode GPU instance type
  - Examples: `g1-gpu-rtx6000-1`, `g1-gpu-a100-1`
  - Source: Linode Marketplace deployment selection
  - Stored in: Not explicitly stored, inferred from instance metadata
- `deployment_timestamp` (datetime): When deployment was initiated
  - Format: ISO 8601 or Unix timestamp
  - Source: StackScript execution time
  - Stored in: `/var/log/ai-sandbox/deployment.log`
- `default_model_identifier` (string): Model used for this deployment
  - Value: `mistralai/Mistral-7B-Instruct-v0.3` (fixed in v1)
  - Stored in: `MODEL_ID` environment variable in docker-compose.yml
- `deployment_status` (enum): Current deployment state
  - Values: `in_progress`, `completed`, `failed`
  - Determined by: StackScript execution, service health
  - Stored in: `/etc/motd` (user-visible), log files

**Storage**:
- Configuration file: `/opt/ai-sandbox/docker-compose.yml` (generated by StackScript)
- Environment variables: In docker-compose.yml
- Status messages: `/etc/motd` (user-visible), `/var/log/ai-sandbox/` (detailed)

**Lifecycle**:
1. **Initiation**: User deploys via Linode Marketplace
2. **StackScript Execution**: Reads UDFs, creates directories, generates docker-compose.yml
3. **Service Launch**: Docker Compose starts containers
4. **Model Download**: vLLM downloads model (if not cached)
5. **Service Ready**: Both services accessible, status written to `/etc/motd`
6. **Runtime**: Services running, auto-restart on failure

**Validation Rules**:
- Instance type must support GPU (validated by Marketplace)
- Model identifier must be valid Hugging Face format
- Deployment must complete within 5 minutes (constitution requirement)
- Services must start successfully (validated by health checks)

---

## Relationships

### AI Model ↔ Chat Session
- **Relationship**: One-to-many
- **Description**: One AI model serves multiple chat sessions
- **Implementation**: Both Open WebUI and API requests use the same vLLM service instance

### Deployment Configuration → AI Model
- **Relationship**: One-to-one (in v1)
- **Description**: Each deployment uses one fixed default model
- **Implementation**: `MODEL_ID` set in docker-compose.yml at deployment time

### Deployment Configuration → Chat Session
- **Relationship**: One-to-many
- **Description**: One deployment supports multiple concurrent chat sessions
- **Implementation**: Open WebUI manages multiple sessions, all served by same model instance

## Data Persistence

### Model Files
- **Location**: `/opt/models` (host directory)
- **Persistence**: Survives container restarts, instance reboots
- **Size**: ~14GB for Mistral-7B-Instruct-v0.3 (approximate)
- **Backup**: Not required (can be re-downloaded from Hugging Face)

### Chat History
- **Location**: `/opt/open-webui` (host directory, mounted to container)
- **Persistence**: Survives container restarts, instance reboots
- **Format**: Open WebUI internal database (SQLite or similar)
- **Backup**: User responsibility (not automated in v1)

### Configuration
- **Location**: `/opt/ai-sandbox/docker-compose.yml`
- **Persistence**: Survives instance reboots (file on host)
- **Regeneration**: Recreated by StackScript on each deployment

## State Transitions

### Model Download Status
```
pending → downloading → completed
                ↓
             failed (retry or error message)
```

### Service Status
```
stopped → starting → running
            ↓          ↓
         failed    stopped (on error, auto-restart)
```

### Deployment Status
```
in_progress → completed (services ready)
      ↓
   failed (error message in /etc/motd)
```

## Data Validation

### Model Identifier
- Must match Hugging Face format: `{org}/{model-name}`
- Must be accessible from Hugging Face (network validation)
- Must be compatible with vLLM (runtime validation)

### Instance Resources
- GPU memory must accommodate model size
- Disk space must accommodate model files (~14GB minimum)
- Network bandwidth must support model download

### Service Health
- API endpoint must respond to health checks
- UI endpoint must serve web interface
- Both services must use same model instance

