# Research: One-Click AI Sandbox

**Date**: 2025-11-12  
**Feature**: One-Click AI Sandbox Marketplace App

## Research Decisions

### 1. OpenAI API v1 Compatibility

**Decision**: Target OpenAI API v1 (latest stable version) for endpoint compatibility.

**Rationale**: 
- OpenAI API v1 is the current stable version used by most applications
- Ensures compatibility with existing applications that use OpenAI SDKs
- Future-proofs the implementation as v1 is actively maintained
- Clarified in spec clarification session

**Alternatives Considered**:
- Legacy API versions: Rejected - not widely used, limits compatibility
- Multiple version support: Rejected - adds complexity, v1 covers all current use cases

**Implementation Notes**: vLLM's OpenAI-compatible server supports v1 format natively.

---

### 2. Concurrent Request Handling

**Decision**: Queue requests and process sequentially (one inference at a time).

**Rationale**:
- Simplest approach for single-instance deployment
- Prevents GPU memory contention
- Ensures consistent response quality
- Avoids complex parallel processing overhead
- Clarified in spec clarification session

**Alternatives Considered**:
- Parallel processing: Rejected - requires more GPU memory, complex queue management
- Rate limiting with rejection: Rejected - poor user experience, doesn't solve resource contention
- Request batching: Rejected - adds latency, complexity for minimal benefit in single-instance scenario

**Implementation Notes**: vLLM supports sequential processing. Queue management handled by API server.

---

### 3. Default Model Selection

**Decision**: Use `mistralai/Mistral-7B-Instruct-v0.3` as the default model.

**Rationale**:
- Matches existing README documentation
- Good balance of quality, speed, and resource requirements
- 7B parameter size fits most GPU instance types
- Well-supported and actively maintained
- Clarified in spec clarification session

**Alternatives Considered**:
- Larger models (13B+): Rejected - may exceed instance memory limits, slower inference
- Smaller models (3B): Rejected - lower quality, minimal resource savings
- Model selection at deployment: Deferred to future release per spec

**Implementation Notes**: Model ID passed via environment variable to vLLM container.

---

### 4. Error Communication Strategy

**Decision**: Display clear error messages in `/etc/motd` and log files with actionable guidance.

**Rationale**:
- `/etc/motd` is immediately visible on SSH login
- Aligns with constitution requirement for user guidance
- Log files provide detailed troubleshooting information
- No dependency on Marketplace UI for error display
- Clarified in spec clarification session

**Alternatives Considered**:
- Silent failure with log-only: Rejected - violates constitution user guidance principle
- Marketplace UI status: Rejected - not always available, adds dependency
- Email notifications: Rejected - requires additional infrastructure, not immediate

**Implementation Notes**: StackScript writes error messages to `/etc/motd` on failure. Detailed logs written to `/var/log/ai-sandbox/`.

---

### 5. Deployment Architecture

**Decision**: Use Linode StackScript with custom "Golden Image" base.

**Rationale**:
- StackScript provides deployment automation on first boot
- Golden Image pre-installs drivers and Docker for faster deployment
- Reduces deployment time (meets 5-minute target)
- Standard Linode Marketplace App pattern
- Aligns with existing documentation

**Alternatives Considered**:
- Pure StackScript (no Golden Image): Rejected - driver installation too slow, exceeds 5-minute target
- Cloud-init scripts: Rejected - Linode Marketplace uses StackScripts
- Manual installation: Rejected - violates "one-click" requirement

**Implementation Notes**: Golden Image includes Ubuntu 22.04 LTS, NVIDIA drivers, Docker Engine, Docker Compose v2.

---

### 6. Container Orchestration

**Decision**: Use Docker Compose v2 for service management.

**Rationale**:
- Required by constitution (Principle V: Maintainability & Observability)
- Industry standard for container orchestration
- Supports GPU passthrough
- Enables automatic restart policies
- Simplifies service management

**Alternatives Considered**:
- Kubernetes: Rejected - overkill for single-instance deployment, adds complexity
- Docker run scripts: Rejected - harder to manage, no automatic restart policies
- Systemd service files: Rejected - less portable, harder to maintain

**Implementation Notes**: `docker-compose.yml` generated by StackScript with restart policies and GPU configuration.

---

### 7. Service Ports

**Decision**: 
- API service: Port 8000 (standard vLLM port)
- UI service: Port 3000 (standard Open WebUI port)

**Rationale**:
- Matches existing README documentation
- Standard ports for these services
- Avoids port conflicts with common services
- Well-documented in service documentation

**Alternatives Considered**:
- Custom ports: Rejected - requires additional configuration, no benefit
- Dynamic port assignment: Rejected - adds complexity, harder to document

**Implementation Notes**: Ports exposed in docker-compose.yml. Security warnings in `/etc/motd` reference these ports.

---

### 8. Model Caching Strategy

**Decision**: Cache models in `/opt/models` host directory, mounted to containers.

**Rationale**:
- Persists across container restarts (FR-008)
- Reduces download time on service restart
- Standard Docker volume pattern
- Aligns with constitution observability requirements

**Alternatives Considered**:
- Container-only storage: Rejected - models re-download on restart, violates FR-008
- Network storage: Rejected - adds complexity, not needed for single-instance

**Implementation Notes**: Host directory created by StackScript, mounted as volume in docker-compose.yml.

---

## Technical Dependencies

### Required Technologies
- **Linode Marketplace App Framework**: StackScript execution environment
- **Ubuntu 22.04 LTS**: Base operating system
- **NVIDIA Drivers**: GPU support (pre-installed in Golden Image)
- **Docker Engine**: Container runtime (pre-installed in Golden Image)
- **Docker Compose v2**: Service orchestration (pre-installed in Golden Image)
- **vLLM**: OpenAI-compatible inference server (containerized)
- **Open WebUI**: Chat interface (containerized)

### External Services
- **Hugging Face**: Model repository for `mistralai/Mistral-7B-Instruct-v0.3`
- **Docker Hub / GHCR**: Container image registry

### Integration Points
- **Linode Marketplace**: Deployment interface
- **StackScript API**: Deployment automation
- **OpenAI API v1**: Compatibility target for API endpoint

## Open Questions Resolved

All technical unknowns from the specification have been resolved through research and clarification:

1. ✅ OpenAI API version: v1 (clarified)
2. ✅ Request handling: Sequential queuing (clarified)
3. ✅ Default model: `mistralai/Mistral-7B-Instruct-v0.3` (clarified)
4. ✅ Error communication: `/etc/motd` + logs (clarified)
5. ✅ Deployment architecture: StackScript + Golden Image (researched)
6. ✅ Container orchestration: Docker Compose v2 (researched)
7. ✅ Service ports: 8000 (API), 3000 (UI) (researched)
8. ✅ Model caching: Host directory mount (researched)

## Next Steps

Proceed to Phase 1: Design & Contracts
- Generate data-model.md
- Generate API contracts
- Generate quickstart.md
- Update agent context

